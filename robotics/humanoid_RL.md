# Humanoid RL Project

## Overview

Minimal scaffold combining Isaac Gym simulation, PyTorch deep RL (PPO), and ROS2 control bridge for a humanoid prototype.

---

# Humanoid RL Research: Irreversible and Novelty-Based Learning Architectures

This repository contains simulations and research experiments for a novel reinforcement learning framework inspired by brain plasticity, irreversible quantum walks, and causal attention. The goal is to enable agentsâ€”particularly humanoid robotsâ€”to learn adaptively without revisiting obsolete behaviors.

## ðŸ” Highlights

- **MiniGrid with Novelty Reward**: Rewards shaped by trajectory novelty and irreversible memory.
- **MuJoCo Hopper**: Suppression-based continuous control with evolving policy shifts.
- **Isaac Gym Humanoid (WIP)**: Transformer policy using forward-planning memory, under irreversible reward shaping.

## ðŸ§  Core Concepts

- **Irreversible Policy Update**: Repeated actions in similar states are suppressed over time.
- **Trajectory Novelty**: Rewards include a penalty for trajectory similarity to prior paths.
- **Causal Planning**: Isaac Gym prototype uses transformer agents that attend to suppressed future state memories.

## ðŸ’» Getting Started

```bash
git clone https://github.com/yourhandle/humanoid-rl-research
cd humanoid-rl-research
pip install -r requirements.txt
```

---

## â–¶ï¸ Run Experiments

```bash
python src/irreversible_rl_simulations.py  
# Runs MiniGrid
# For MuJoCo:
# Uncomment train_mujoco() in the script
```

---

## ðŸ“ Citation
Please cite our upcoming publication or reference the repository directly:

```bibtex
@misc{irreversible_rl,
  title={Irreversible Reinforcement Learning for Adaptive Control},
  author={Goolam Hossen, Yashine},
  year={2025},
  note={https://github.com/yourhandle/humanoid-rl-research}
}
```

---


## ðŸ§ª Acknowledgements
Developed as part of an independent doctoral-level research project combining quantum information, reinforcement learning, and humanoid robotics.



---

### Directory Tree

```
humanoid-rl-research/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ CITATION.cff
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ irreversible_rl_simulations.py  # All simulations
â”‚   â”œâ”€â”€ train_isaac_humanoid.py  
â”‚   â”œâ”€â”€ agent_utils.py                  # Optional modular helpers
â”‚   â”œâ”€â”€ config.yaml                     # Configs if used
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ results_analysis.ipynb          # Plots, analysis
â”‚
â”œâ”€â”€ results/
â”‚   â””â”€â”€ minigrid/
â”‚   â””â”€â”€ mujoco/
â”‚   â””â”€â”€ isaac/
â”‚
â””â”€â”€ gifs/
    â””â”€â”€ demo.gif
```


---

```
humanoid_rl/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ ros2_ws/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ humanoid_control/
â”‚   â”‚       â”œâ”€â”€ package.xml
â”‚   â”‚       â”œâ”€â”€ setup.py
â”‚   â”‚       â””â”€â”€ humanoid_control/
â”‚   â”‚           â”œâ”€â”€ __init__.py
â”‚   â”‚           â””â”€â”€ control_node.py
â””â”€â”€ python/
    â”œâ”€â”€ envs/
    â”‚   â””â”€â”€ humanoid_env.py
    â””â”€â”€ train_policy.py
```

---

### requirements.txt

```text
isaacgym==0.2.3
torch>=2.1
stable-baselines3==2.3.0
gym==0.26
rclpy
numpy
```

---

### README.md (excerpt)

```markdown
# Setup
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# Build ROS2 workspace
cd ros2_ws && colcon build
source install/setup.bash
```

---

### python/envs/humanoid\_env.py (simplified)

```python
import torch
from isaacgym import gymapi
from isaacgym import gymtorch
import gym
from gym import spaces

class HumanoidEnv(gym.Env):
    def __init__(self, cfg):
        self.gym = gymapi.acquire_gym()
        # create sim, load asset, etc.
        # ...
        self.obs_space = spaces.Box(-1.0, 1.0, (cfg['obs_dim'],), dtype=float)
        self.act_space = spaces.Box(-1.0, 1.0, (cfg['act_dim'],), dtype=float)

    def step(self, action):
        # apply forces to humanoid joints
        # ...
        reward = self.compute_reward()
        done = self.check_terminate()
        return self.get_obs(), reward, done, {}

    def reset(self):
        # reset sim
        # ...
        return self.get_obs()

    def get_obs(self):
        # return concatenated joint angles, velocities, root pose
        return self.obs_buf
```

---

### python/train\_policy.py (simplified)

```python
import torch
from stable_baselines3 import PPO
from envs.humanoid_env import HumanoidEnv

cfg = dict(obs_dim=120, act_dim=28)
env = HumanoidEnv(cfg)
model = PPO("MlpPolicy", env, verbose=1, n_steps=2048, batch_size=512,
            tensorboard_log="./tb")

model.learn(total_timesteps=3_000_000)
model.save("ppo_humanoid")
```

---

### ros2\_ws/src/humanoid\_control/control\_node.py

```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import JointState
import torch

class HumanoidControl(Node):
    def __init__(self):
        super().__init__('humanoid_control')
        self.pub = self.create_publisher(JointState, '/humanoid/joint_cmd', 10)
        self.model = torch.jit.load('ppo_humanoid.pt')
        self.create_timer(0.02, self.control_callback)  # 50 Hz

    def control_callback(self):
        # TODO: subscribe to /humanoid/joint_states and sensor topics
        obs = self.get_obs()
        action, _ = self.model.predict(obs, deterministic=False)
        msg = JointState()
        msg.position = action.tolist()
        self.pub.publish(msg)

    def get_obs(self):
        # TODO: assemble observation vector from sensors
        return []


def main():
    rclpy.init()
    node = HumanoidControl()
    rclpy.spin(node)

if __name__ == '__main__':
    main()
```

---

## Quick Run

1. **Train in simulation**

   ```bash
   python python/train_policy.py
   ```
2. **Launch ROS2 control node**

   ```bash
   source ros2_ws/install/setup.bash
   ros2 run humanoid_control control_node
   ```

---

   ```bash
python train_isaac_humanoid.py
```

## ðŸ§ª What It Does:
Loads HumanoidTask with 2048 parallel agents

Applies transformer policy on 8-step observation sequences

Injects novelty suppression based on visit frequency of state patterns

Prints reward every 100 steps

---





---

## Skill Tests

* **Adaptability:** Randomize terrain in env and retrain.
* **Failâ€‘safe:** Disable one joint midâ€‘episode, maintain balance.
* **Latency:** Measure ROS2 roundâ€‘trip latencies (goal <20â€¯ms).
